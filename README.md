# Mini Projet Reinforcement Learning

Projet de reinforcement learning avec environnement GridWorld dynamique, agent random, agent state value et **agent Q-Learning**.

## Structure du Projet

- `environment.py` - Environnement GridWorld type OpenAI Gym avec **objectif mobile**
- `agent_random.py` - Agent qui choisit des actions al√©atoires
- `agent_state_value.py` - Agent utilisant Value Iteration (pour environnements statiques)
- `agent_q_learning.py` - **Agent utilisant Q-Learning (adapt√© aux environnements dynamiques)**
- `main.py` - Script principal pour entra√Æner et comparer les agents

## Nouveaut√©s

### üéØ Objectif Mobile
L'environnement peut maintenant avoir un objectif qui se d√©place :
- **√Ä chaque √©pisode** : nouvelle position al√©atoire
- **Pendant l'√©pisode** : changement toutes les N steps (configurable)

### üß† Agent Q-Learning
- Apprend la fonction Q optimale Q(s,a)
- Exploration epsilon-greedy avec d√©croissance
- S'adapte aux environnements dynamiques
- Convergence progressive vers la politique optimale

## Installation

```bash
pip install numpy matplotlib
```

## Utilisation

Ex√©cutez simplement le script principal :

```bash
python main.py
```

## Configuration

Dans `main.py`, vous pouvez modifier les param√®tres suivants :

### Param√®tres de l'environnement
- `GRID_SIZE` : Taille de la grille (par d√©faut : 5)
- `GOAL_REWARD` : R√©compense pour atteindre l'objectif (par d√©faut : 10)
- `STEP_PENALTY` : P√©nalit√© pour chaque pas (par d√©faut : -0.1)
- `OBSTACLE_PENALTY` : P√©nalit√© pour toucher un obstacle (par d√©faut : -5)
- `MOVING_GOAL` : **Activer l'objectif mobile** (par d√©faut : True)
- `GOAL_MOVE_INTERVAL` : **Nombre de pas avant d√©placement de l'objectif** (par d√©faut : 10)

### Param√®tres d'apprentissage
- `GAMMA` : Facteur de discount (par d√©faut : 0.9)
- `ALPHA` : **Taux d'apprentissage pour Q-Learning** (par d√©faut : 0.1)
- `EPSILON` : **Taux d'exploration initial pour Q-Learning** (par d√©faut : 0.3)
- `NUM_EPISODES_RANDOM` : Nombre d'√©pisodes pour l'agent random (par d√©faut : 100)
- `NUM_EPISODES_Q_LEARNING` : **Nombre d'√©pisodes pour Q-Learning** (par d√©faut : 1000)
- `NUM_EPISODES_EVAL` : Nombre d'√©pisodes pour l'√©valuation (par d√©faut : 10)
- `MAX_STEPS` : Nombre maximum de pas par √©pisode (par d√©faut : 100)

## Fonctionnalit√©s

### Environnement GridWorld Dynamique
- Grille personnalisable
- Obstacles statiques
- Position de d√©part (S) fixe
- **Objectif (G) mobile** :
  - Nouvelle position al√©atoire √† chaque √©pisode
  - Peut se d√©placer pendant l'√©pisode
- R√©compenses configurables
- Visualisation avec matplotlib

### Agent Random
- S√©lection d'actions al√©atoires
- Utilis√© comme baseline de comparaison

### Agent Q-Learning ‚≠ê
- **Algorithme** : Q-Learning (temporal difference)
- **Table Q** : Stocke Q(s,a) pour chaque paire √©tat-action
- **Politique** : Epsilon-greedy avec d√©croissance
- **Update Rule** : Q(s,a) ‚Üê Q(s,a) + Œ±[r + Œ≥ max Q(s',a') - Q(s,a)]
- **Avantages** :
  - S'adapte aux environnements dynamiques
  - Pas besoin de mod√®le de l'environnement
  - Apprentissage en ligne

### Agent State Value
- Algorithme : Value Iteration (programmation dynamique)
- Apprend la fonction de valeur optimale V*(s)
- Extrait une politique optimale
- **Note** : Fonctionne mieux sur environnements statiques
- Converge vers la solution optimale

## Visualisation

Le programme g√©n√®re plusieurs graphiques :

1. **environment_visualization.png** : 
   - Environnement de base
   - Fonction Q et politique Q-Learning (fl√®ches)
   - Trajectoire optimale Q-Learning
   - [Fonction V et trajectoire State Value si objectif statique]

2. **training_results.png** :
   - Performance de l'agent random
   - Courbe d'apprentissage Q-Learning
   - [√âvaluation State Value si objectif statique]

3. **Animations interactives** :
   - Mouvement en temps r√©el de Q-Learning
   - Mouvement en temps r√©el de Random
   - [Mouvement de State Value si objectif statique]

## Comparaison des Algorithmes

| Algorithme | Type | Environnement | Convergence | Complexit√© |
|------------|------|---------------|-------------|------------|
| **Q-Learning** | Model-free TD | Statique/Dynamique | Progressive | O(S√óA) |
| **Value Iteration** | Model-based DP | Statique | Rapide | O(S¬≤√óA) |
| **Random** | Baseline | Tous | Aucune | O(1) |

## Actions

- 0 : Haut ‚Üë
- 1 : Droite ‚Üí
- 2 : Bas ‚Üì
- 3 : Gauche ‚Üê

## Exemples de Configuration

### Environnement Dynamique (par d√©faut)
```python
MOVING_GOAL = True
GOAL_MOVE_INTERVAL = 10
NUM_EPISODES_Q_LEARNING = 1000
ALPHA = 0.1
EPSILON = 0.3
```

### Environnement Statique
```python
MOVING_GOAL = False
# L'agent State Value sera √©galement entra√Æn√©
```

### Grille Plus Grande
```python
GRID_SIZE = 10
GOAL_REWARD = 20
STEP_PENALTY = -0.5
OBSTACLE_PENALTY = -10
```

## R√©sultats Attendus

- **Agent Random** : Performance erratique, pas d'apprentissage
- **Agent Q-Learning** : Am√©lioration progressive, convergence vers politique optimale
- **Agent State Value** : Performance optimale d√®s l'entra√Ænement (si statique)

L'agent Q-Learning devrait significativement surpasser l'agent Random, surtout apr√®s plusieurs √©pisodes d'entra√Ænement. Dans un environnement statique, State Value converge plus rapidement mais Q-Learning finit par atteindre une performance similaire.
