"""
Page 4 - D√©ploiement et inf√©rence temps r√©el.
"""

import streamlit as st
import sys
from pathlib import Path

# Ajouter le r√©pertoire src au path
sys.path.append(str(Path(__file__).parent.parent / "src"))


def setup_page():
    """Configuration de la page."""
    st.set_page_config(
        page_title="D√©ploiement - Teachable Machine",
        page_icon="üöÄ",
        layout="wide"
    )


def check_model_available():
    """V√©rifie si un mod√®le est disponible."""
    if not st.session_state.get('model_trained') and not st.session_state.get('selected_model_path'):
        st.error("‚ùå Aucun mod√®le disponible")
        st.markdown("Veuillez d'abord entra√Æner un mod√®le ou charger un mod√®le existant.")
        
        col1, col2 = st.columns(2)
        with col1:
            if st.button("üß™ Entra√Æner un mod√®le"):
                st.switch_page("pages/2_üß™_Experiment_&_Train.py")
        with col2:
            if st.button("üìÅ Charger un mod√®le existant"):
                load_existing_model()
        return False
    return True


def load_existing_model():
    """Interface pour charger un mod√®le existant."""
    st.subheader("üìÇ Charger un mod√®le existant")
    
    # Lister les mod√®les disponibles
    artifacts_dir = Path("artifacts")
    if artifacts_dir.exists():
        model_runs = sorted(list(artifacts_dir.glob("run_*")), reverse=True)  # Plus r√©cents en premier
        
        if model_runs:
            # Affichage d√©taill√© des mod√®les disponibles
            st.write("**Mod√®les disponibles :**")
            
            selected_run = None
            
            for run_dir in model_runs:
                with st.container():
                    col1, col2, col3, col4 = st.columns([2, 2, 2, 1])
                    
                    # Lire les m√©tadonn√©es si disponibles
                    metadata_file = run_dir / "metadata.json"
                    if metadata_file.exists():
                        try:
                            import json
                            with open(metadata_file, 'r') as f:
                                metadata = json.load(f)
                            
                            with col1:
                                st.write(f"**{run_dir.name}**")
                                timestamp = metadata.get('timestamp', '')
                                if timestamp:
                                    from datetime import datetime
                                    dt = datetime.fromisoformat(timestamp.replace('Z', '+00:00'))
                                    st.caption(dt.strftime("%d/%m/%Y %H:%M"))
                            
                            with col2:
                                dataset_name = metadata.get('dataset_name', 'N/A')
                                task_type = metadata.get('task_type', 'N/A')
                                st.write(f"üìä {dataset_name}")
                                st.caption(f"Type: {task_type}")
                            
                            with col3:
                                final_metrics = metadata.get('final_metrics', {})
                                accuracy = final_metrics.get('accuracy', 0)
                                epochs = final_metrics.get('epochs_completed', 0)
                                st.metric("Accuracy", f"{accuracy:.1%}")
                                st.caption(f"{epochs} epochs")
                            
                            with col4:
                                if st.button("üì• Charger", key=f"load_{run_dir.name}"):
                                    selected_run = run_dir.name
                                    
                        except Exception as e:
                            with col1:
                                st.write(f"**{run_dir.name}**")
                                st.caption("M√©tadonn√©es corrompues")
                            with col4:
                                if st.button("üì• Charger", key=f"load_{run_dir.name}"):
                                    selected_run = run_dir.name
                    else:
                        # Ancien format sans m√©tadonn√©es
                        with col1:
                            st.write(f"**{run_dir.name}**")
                            st.caption("Ancien format")
                        with col2:
                            # V√©rifier si le fichier mod√®le existe
                            model_file = run_dir / "model.keras"
                            if model_file.exists():
                                st.write("‚úÖ Mod√®le pr√©sent")
                            else:
                                st.write("‚ùå Mod√®le manquant")
                        with col4:
                            if st.button("üì• Charger", key=f"load_{run_dir.name}"):
                                selected_run = run_dir.name
                    
                    st.markdown("---")
            
            # Charger le mod√®le s√©lectionn√©
            if selected_run:
                model_path = artifacts_dir / selected_run / "model.keras"
                st.session_state.selected_model_path = str(model_path)
                st.session_state.model_trained = True
                
                # Charger aussi les m√©tadonn√©es dans la session
                metadata_file = artifacts_dir / selected_run / "metadata.json"
                if metadata_file.exists():
                    try:
                        import json
                        with open(metadata_file, 'r') as f:
                            metadata = json.load(f)
                        st.session_state.loaded_model_metadata = metadata
                    except:
                        pass
                
                st.success(f"‚úÖ Mod√®le {selected_run} charg√© avec succ√®s !")
                st.info(f"üìÅ Chemin: `{model_path}`")
                st.rerun()
                
        else:
            st.info("üîç Aucun mod√®le sauvegard√© trouv√©")
            st.markdown("Entra√Ænez d'abord un mod√®le dans l'onglet **üß™ Experiment & Train**")
    else:
        st.warning("üìÅ Dossier `artifacts/` non trouv√©")
        st.markdown("Le dossier sera cr√©√© automatiquement lors du premier entra√Ænement.")


def realtime_inference():
    """Interface d'inf√©rence en temps r√©el."""
    st.subheader("üéØ Inf√©rence en temps r√©el")
    
    tab1, tab2, tab3, tab4 = st.tabs(["Upload Image", "Webcam", "Dossier", "URL"])
    
    with tab1:
        single_image_inference()
    
    with tab2:
        webcam_inference()
    
    with tab3:
        batch_inference()
    
    with tab4:
        url_inference()


def single_image_inference():
    """Inf√©rence sur une image upload√©e."""
    st.markdown("**Upload d'une image**")
    
    uploaded_file = st.file_uploader(
        "S√©lectionnez une image",
        type=['jpg', 'jpeg', 'png', 'bmp'],
        help="Formats support√©s : JPG, PNG, BMP"
    )
    
    if uploaded_file is not None:
        # Afficher l'image
        col1, col2 = st.columns(2)
        
        with col1:
            st.image(uploaded_file, caption="Image upload√©e", use_column_width=True)
        
        with col2:
            if st.button("üîç Pr√©dire", use_container_width=True):
                with st.spinner("Pr√©diction en cours..."):
                    # Simuler une pr√©diction
                    result = simulate_prediction()
                    display_prediction_result(result)


def webcam_inference():
    """Inf√©rence via webcam."""
    st.markdown("**Webcam en temps r√©el**")
    st.info("üöß Fonctionnalit√© webcam en cours d'impl√©mentation")
    
    # Placeholder pour l'interface webcam
    enable_webcam = st.checkbox("Activer la webcam")
    
    if enable_webcam:
        st.info("La webcam serait ici avec cv2 et streamlit-webrtc")
        
        if st.button("üì∏ Capturer et pr√©dire"):
            result = simulate_prediction()
            display_prediction_result(result)


def batch_inference():
    """Inf√©rence sur un dossier d'images."""
    st.markdown("**Traitement par lot**")
    
    folder_path = st.text_input(
        "Chemin du dossier d'images",
        placeholder="/path/to/images"
    )
    
    if folder_path and st.button("üìÅ Traiter le dossier"):
        folder_path = Path(folder_path)
        
        if folder_path.exists():
            # Simuler le traitement par lot
            with st.spinner("Traitement des images..."):
                # Simuler la d√©couverte d'images
                image_extensions = ['.jpg', '.jpeg', '.png', '.bmp']
                images = []
                for ext in image_extensions:
                    images.extend(folder_path.glob(f"*{ext}"))
                    images.extend(folder_path.glob(f"*{ext.upper()}"))
                
                if images:
                    st.success(f"Trouv√© {len(images)} images")
                    
                    # Simuler les pr√©dictions
                    results = []
                    progress_bar = st.progress(0)
                    
                    for i, img_path in enumerate(images[:10]):  # Limiter √† 10 pour la d√©mo
                        result = simulate_prediction(str(img_path))
                        results.append(result)
                        progress_bar.progress((i + 1) / min(len(images), 10))
                    
                    # Afficher les r√©sultats
                    display_batch_results(results)
                    
                    # Option d'export
                    if st.button("üíæ Exporter les r√©sultats (CSV)"):
                        st.success("R√©sultats export√©s vers predictions.csv")
                else:
                    st.warning("Aucune image trouv√©e dans le dossier")
        else:
            st.error("Le dossier sp√©cifi√© n'existe pas")


def url_inference():
    """Inf√©rence sur une image depuis une URL."""
    st.markdown("**Image depuis URL**")
    
    image_url = st.text_input(
        "URL de l'image",
        placeholder="https://example.com/image.jpg"
    )
    
    if image_url and st.button("üåê Charger et pr√©dire"):
        try:
            # Ici on chargerait r√©ellement l'image depuis l'URL
            st.info(f"Chargement depuis : {image_url}")
            
            # Simuler le chargement et la pr√©diction
            result = simulate_prediction(image_url)
            display_prediction_result(result)
            
        except Exception as e:
            st.error(f"Erreur lors du chargement : {e}")


def simulate_prediction(image_path="uploaded_image"):
    """Simule une pr√©diction."""
    import random
    import time
    
    # Simuler le temps de traitement
    time.sleep(random.uniform(0.5, 1.5))
    
    dataset_info = st.session_state.get('dataset_info', None)
    task_type = getattr(dataset_info, 'task_type', 'classification') if dataset_info else 'classification'
    
    if task_type == "classification":
        # Simuler une pr√©diction de classification
        class_names = getattr(dataset_info, 'class_names', ['Classe A', 'Classe B', 'Classe C']) if dataset_info else ['Classe A', 'Classe B', 'Classe C']
        predictions = [random.random() for _ in class_names]
        # Normaliser pour que la somme soit 1
        total = sum(predictions)
        predictions = [p / total for p in predictions]
        
        predicted_class = class_names[predictions.index(max(predictions))]
        confidence = max(predictions)
        
        return {
            'type': 'classification',
            'predicted_class': predicted_class,
            'confidence': confidence,
            'all_predictions': dict(zip(class_names, predictions)),
            'image_path': image_path,
            'processing_time': random.uniform(0.1, 0.3)
        }
    
    else:  # regression
        # Simuler une pr√©diction de r√©gression
        target_range = getattr(dataset_info, 'target_range', (0, 100)) if dataset_info else (0, 100)
        predicted_value = random.uniform(target_range[0], target_range[1])
        
        return {
            'type': 'regression',
            'predicted_value': predicted_value,
            'image_path': image_path,
            'processing_time': random.uniform(0.1, 0.3)
        }


def display_prediction_result(result):
    """Affiche le r√©sultat d'une pr√©diction."""
    st.subheader("üéØ R√©sultat de la pr√©diction")
    
    col1, col2 = st.columns(2)
    
    with col1:
        st.metric("Temps de traitement", f"{result['processing_time']:.3f}s")
    
    if result['type'] == 'classification':
        with col2:
            st.metric("Classe pr√©dite", result['predicted_class'])
            st.metric("Confiance", f"{result['confidence']:.3f}")
        
        # Afficher toutes les probabilit√©s
        st.markdown("**Probabilit√©s par classe :**")
        for class_name, prob in result['all_predictions'].items():
            st.write(f"‚Ä¢ {class_name}: {prob:.3f}")
        
        # Graphique des probabilit√©s
        import pandas as pd
        prob_df = pd.DataFrame({
            'Classe': list(result['all_predictions'].keys()),
            'Probabilit√©': list(result['all_predictions'].values())
        })
        st.bar_chart(prob_df.set_index('Classe'))
    
    else:  # regression
        with col2:
            st.metric("Valeur pr√©dite", f"{result['predicted_value']:.3f}")


def display_batch_results(results):
    """Affiche les r√©sultats du traitement par lot."""
    st.subheader("üìä R√©sultats du traitement par lot")
    
    # Statistiques g√©n√©rales
    col1, col2, col3 = st.columns(3)
    
    with col1:
        st.metric("Images trait√©es", len(results))
    with col2:
        avg_time = sum(r['processing_time'] for r in results) / len(results)
        st.metric("Temps moyen", f"{avg_time:.3f}s")
    with col3:
        total_time = sum(r['processing_time'] for r in results)
        st.metric("Temps total", f"{total_time:.1f}s")
    
    # Tableau des r√©sultats
    if results and results[0]['type'] == 'classification':
        # Classification
        import pandas as pd
        
        data = []
        for result in results:
            data.append({
                'Image': Path(result['image_path']).name,
                'Classe pr√©dite': result['predicted_class'],
                'Confiance': f"{result['confidence']:.3f}",
                'Temps': f"{result['processing_time']:.3f}s"
            })
        
        df = pd.DataFrame(data)
        st.dataframe(df, use_container_width=True)
    
    else:
        # R√©gression
        import pandas as pd
        
        data = []
        for result in results:
            data.append({
                'Image': Path(result['image_path']).name,
                'Valeur pr√©dite': f"{result['predicted_value']:.3f}",
                'Temps': f"{result['processing_time']:.3f}s"
            })
        
        df = pd.DataFrame(data)
        st.dataframe(df, use_container_width=True)


def model_export():
    """Section d'export du mod√®le."""
    st.subheader("üì¶ Export du mod√®le")
    
    col1, col2, col3 = st.columns(3)
    
    with col1:
        st.markdown("**Format Keras**")
        st.info("Format natif TensorFlow/Keras")
        if st.button("üíæ Exporter .keras"):
            st.success("Mod√®le export√© en format .keras")
    
    with col2:
        st.markdown("**Format ONNX**")
        st.info("Format optimis√© multi-plateformes")
        if st.button("üîÑ Convertir en ONNX"):
            with st.spinner("Conversion en cours..."):
                # Simuler la conversion
                import time
                time.sleep(2)
                st.success("Mod√®le converti en ONNX")
    
    with col3:
        st.markdown("**TensorFlow Lite**")
        st.info("Format mobile optimis√©")
        if st.button("üì± Convertir en TFLite"):
            with st.spinner("Optimisation pour mobile..."):
                import time
                time.sleep(1.5)
                st.success("Mod√®le optimis√© en TFLite")


def api_generation():
    """G√©n√©ration d'API FastAPI."""
    st.subheader("üîó G√©n√©ration d'API")
    
    st.markdown("Cr√©ez automatiquement une API REST pour votre mod√®le")
    
    # Configuration de l'API
    with st.expander("‚öôÔ∏è Configuration de l'API"):
        api_name = st.text_input("Nom de l'API", value="teachable_model_api")
        api_port = st.number_input("Port", value=8000, min_value=1000, max_value=9999)
        enable_docs = st.checkbox("Documentation Swagger", value=True)
        enable_cors = st.checkbox("CORS activ√©", value=True)
        
        # Options avanc√©es
        max_file_size = st.slider("Taille max fichier (MB)", 1, 50, 10)
        rate_limiting = st.checkbox("Limitation de d√©bit", value=False)
    
    if st.button("üöÄ G√©n√©rer l'API FastAPI"):
        with st.spinner("G√©n√©ration du code API..."):
            # Simuler la g√©n√©ration
            import time
            time.sleep(1)
            
            api_code = generate_fastapi_code(api_name, api_port)
            
            st.success("API g√©n√©r√©e avec succ√®s !")
            
            # Afficher le code g√©n√©r√©
            with st.expander("üìÑ Code g√©n√©r√© (serve_api.py)"):
                st.code(api_code, language="python")
            
            # Instructions d'utilisation
            st.markdown("**Instructions d'utilisation :**")
            st.code(f"""
# D√©marrer l'API
python serve_api.py

# Tester avec curl
curl -X POST "http://localhost:{api_port}/predict" \\
     -H "Content-Type: multipart/form-data" \\
     -F "file=@image.jpg"

# Documentation disponible sur :
# http://localhost:{api_port}/docs
            """)


def generate_fastapi_code(api_name, port):
    """G√©n√®re le code FastAPI."""
    return f'''
from fastapi import FastAPI, File, UploadFile, HTTPException
from fastapi.middleware.cors import CORSMiddleware
import tensorflow as tf
import numpy as np
from PIL import Image
import io
import uvicorn

app = FastAPI(title="{api_name}", version="1.0.0")

# Configuration CORS
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Charger le mod√®le
model = tf.keras.models.load_model("model.keras")

@app.post("/predict")
async def predict(file: UploadFile = File(...)):
    """Pr√©diction sur une image upload√©e."""
    try:
        # Lire l'image
        contents = await file.read()
        image = Image.open(io.BytesIO(contents))
        
        # Pr√©processing
        image = image.resize((224, 224))
        image_array = np.array(image) / 255.0
        image_array = np.expand_dims(image_array, axis=0)
        
        # Pr√©diction
        prediction = model.predict(image_array)
        
        return {{
            "filename": file.filename,
            "prediction": prediction.tolist()
        }}
        
    except Exception as e:
        raise HTTPException(status_code=400, detail=str(e))

if __name__ == "__main__":
    uvicorn.run(app, host="0.0.0.0", port={port})
'''


def deployment_guide():
    """Guide de d√©ploiement."""
    st.subheader("üìñ Guide de d√©ploiement")
    
    tab1, tab2, tab3 = st.tabs(["Local", "Cloud", "Mobile"])
    
    with tab1:
        st.markdown("""
        **D√©ploiement local :**
        
        1. Exportez votre mod√®le au format souhait√©
        2. G√©n√©rez l'API FastAPI
        3. Installez les d√©pendances :
        ```bash
        pip install fastapi uvicorn tensorflow pillow
        ```
        4. Lancez l'API :
        ```bash
        python serve_api.py
        ```
        """)
    
    with tab2:
        st.markdown("""
        **D√©ploiement cloud :**
        
        **Docker :**
        ```dockerfile
        FROM python:3.9-slim
        COPY requirements.txt .
        RUN pip install -r requirements.txt
        COPY . .
        CMD ["python", "serve_api.py"]
        ```
        
        **Plateforme recommand√©es :**
        - üê≥ Docker + AWS ECS/Google Cloud Run
        - ‚ö° Vercel/Netlify pour les APIs l√©g√®res
        - üöÄ Heroku pour un d√©ploiement rapide
        """)
    
    with tab3:
        st.markdown("""
        **D√©ploiement mobile :**
        
        1. Convertissez en TensorFlow Lite
        2. Int√©grez dans votre app mobile :
        
        **Android (Java/Kotlin) :**
        ```java
        // Charger le mod√®le TFLite
        Interpreter tflite = new Interpreter(loadModelFile());
        ```
        
        **iOS (Swift) :**
        ```swift
        // Utiliser Core ML ou TensorFlow Lite
        let interpreter = try Interpreter(modelPath: modelPath)
        ```
        """)


def main():
    """Fonction principale de la page."""
    setup_page()
    
    st.title("üöÄ D√©ploiement et inf√©rence temps r√©el")
    st.markdown("D√©ployez votre mod√®le et testez-le en conditions r√©elles")
    
    # V√©rifier qu'un mod√®le est disponible
    if not check_model_available():
        return
    
    # Informations sur le mod√®le charg√©
    st.info("‚úÖ Mod√®le pr√™t pour l'inf√©rence")
    
    # Interface d'inf√©rence
    realtime_inference()
    
    st.markdown("---")
    
    # Export du mod√®le
    model_export()
    
    st.markdown("---")
    
    # G√©n√©ration d'API
    api_generation()
    
    st.markdown("---")
    
    # Guide de d√©ploiement
    deployment_guide()


if __name__ == "__main__":
    main()
